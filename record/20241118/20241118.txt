{"论文摘要": "提出了一种基于强化学习的迭代训练方法，用于无监督语音识别中的边界分割，显著提升了无监督语音识别的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[REBORN: Reinforcement-Learned Boundary Segmentation with Iterative  Training for Unsupervised ASR](http://arxiv.org/abs/2402.03988v3)", "领域": {"语音理解": ["speech recognition", "asr"]}}
{"论文摘要": "提出了一种基于特征解耦和增强的零样本歌唱声音转换模型SaMoye，实现了从歌唱声音到人类和非人类音色的转换。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature  Disentanglement and Enhancement](http://arxiv.org/abs/2407.07728v5)", "领域": {"语音理解": ["asr"], "语音生成": ["voice conversion"]}}
{"论文摘要": "提出了一种基于低秩匹配注意力的轻量级跨模态特征融合方法，用于对话情感识别，有效捕捉情感语义信息并降低计算复杂度。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[A Low-rank Matching Attention based Cross-modal Feature Fusion Method  for Conversational Emotion Recognition](http://arxiv.org/abs/2306.17799v2)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出Seed-VC，一种基于扩散Transformer的零样本语音转换框架，有效解决传统方法的音色泄漏、表示不足和训练与推理任务不匹配问题。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 5, "part5分数": 4, "part6分数": 4, "推荐分数": 4.55, "标题": "[Zero-shot Voice Conversion with Diffusion Transformers](http://arxiv.org/abs/2411.09943v1)", "领域": {"语音生成": ["voice conversion"], "大模型": ["transformer"]}}
{"论文摘要": "提出了一种名为TraceableSpeech的新型文本到语音模型，该模型通过直接生成带水印的语音，提高了水印的不可感知性和语音质量，并增强了对抗拼接攻击的鲁棒性和操作的时间灵活性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[TraceableSpeech: Towards Proactively Traceable Text-to-Speech with  Watermarking](http://arxiv.org/abs/2406.04840v3)", "领域": {"语音生成": ["TTS"]}}
{"论文摘要": "提出了一种基于双列架构和自监督学习的Mamba模型，用于检测语音欺骗攻击，并在多个数据集上取得了优异的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing  Attack Detection](http://arxiv.org/abs/2411.10027v1)", "领域": {"语音理解": ["speech recognition"], "大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种名为TraceableSpeech的新型文本到语音模型，该模型通过直接生成带水印的语音，提高了水印的不可感知性和语音质量，同时增强了对抗拼接攻击的鲁棒性和操作的时序灵活性。", "part1分数": "4", "part2分数": "4", "part3分数": "4", "part4分数": "4", "part5分数": "4", "part6分数": "4", "推荐分数": 4.0, "标题": "[TraceableSpeech: Towards Proactively Traceable Text-to-Speech with  Watermarking](http://arxiv.org/abs/2406.04840v3)", "领域": {"语音生成": ["TTS"]}}
{"论文摘要": "提出了一种简单但强大的声音视频生成基线，通过有效融合音频和视频扩散模型，并引入时间步调整和跨模态条件位置编码机制，实现了音频和视频的联合生成。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[A Simple but Strong Baseline for Sounding Video Generation: Effective  Adaptation of Audio and Video Diffusion Models for Joint Generation](http://arxiv.org/abs/2409.17550v2)", "领域": {"大模型": ["attention mechanism"]}}
{"论文摘要": "提出了一种基于双列架构的Mamba模型，用于检测语音伪造攻击，并通过自监督学习和预训练模型提高性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing  Attack Detection](http://arxiv.org/abs/2411.10027v1)", "领域": {"语音理解": ["speech recognition"], "大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种基于强化学习的迭代训练方法，用于无监督语音识别中的边界分割，显著提升了无监督语音识别的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[REBORN: Reinforcement-Learned Boundary Segmentation with Iterative  Training for Unsupervised ASR](http://arxiv.org/abs/2402.03988v3)", "领域": {"语音理解": ["speech recognition", "asr"]}}
{"论文摘要": "VeriGraph通过整合视觉语言模型并使用场景图进行动作可行性验证，提高了机器人任务规划的成功率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[VeriGraph: Scene Graphs for Execution Verifiable Robot Planning](http://arxiv.org/abs/2411.10446v1)", "领域": {"大模型": ["LLM"]}}
{"论文摘要": "提出了一种名为低秩匹配注意力方法（LMAM）的轻量级跨模态特征融合方法，用于对话情感识别，有效捕捉语境情感语义信息并降低计算复杂度。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[A Low-rank Matching Attention based Cross-modal Feature Fusion Method  for Conversational Emotion Recognition](http://arxiv.org/abs/2306.17799v2)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种名为Seed-VC的新型零样本语音转换框架，通过引入外部音色转换器和使用扩散变换器来提高转换质量。", "part1分数": "4", "part2分数": "4", "part3分数": "4", "part4分数": "4", "part5分数": "4", "part6分数": "4", "推荐分数": 4.0, "标题": "[Zero-shot Voice Conversion with Diffusion Transformers](http://arxiv.org/abs/2411.09943v1)", "领域": {"语音生成": ["voice conversion"], "大模型": ["transformer"]}}
{"论文摘要": "提出了一种基于特征解耦和增强的零样本歌唱声音转换模型SaMoye，实现了从歌唱声音到人类和非人类音色的转换。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature  Disentanglement and Enhancement](http://arxiv.org/abs/2407.07728v5)", "领域": {"语音理解": ["asr"], "语音生成": ["voice conversion"]}}
{"论文摘要": "提出MARS优化框架，通过结合预条件梯度方法和方差缩减技术，显著提升大型模型的训练效率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[MARS: Unleashing the Power of Variance Reduction for Training Large  Models](http://arxiv.org/abs/2411.10438v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种简单但强大的声音视频生成基线，通过有效融合音频和视频扩散模型，并引入时间步调整和跨模态条件作为位置编码的新机制，提高了音频视频对齐的准确性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[A Simple but Strong Baseline for Sounding Video Generation: Effective  Adaptation of Audio and Video Diffusion Models for Joint Generation](http://arxiv.org/abs/2409.17550v2)", "领域": {"大模型": ["attention mechanism"]}}
{"论文摘要": "提出了一种针对多模态大型语言模型幻觉问题的针对性直接偏好优化方法，有效降低了幻觉发生的概率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Mitigating Hallucination in Multimodal Large Language Model via  Hallucination-targeted Direct Preference Optimization](http://arxiv.org/abs/2411.10436v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "构建了一个基于Balderdash游戏的模拟框架，用于评估大型语言模型在创意和欺骗性推理方面的能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Evaluating Creativity and Deception in Large Language Models: A  Simulation Framework for Multi-Agent Balderdash](http://arxiv.org/abs/2411.10422v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "本研究提出了一种基于大语言模型的可解释机器学习控制框架，以增强建筑能源系统中的机器学习控制的透明度和可信度。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Large Language Model-Based Interpretable Machine Learning Control in  Building Energy Systems](http://arxiv.org/abs/2402.09584v2)", "领域": {"大模型": ["large language model", "LLM", "LLMs"]}}
{"论文摘要": "探索大型语言模型在建筑能耗模拟中的应用，并通过案例研究展示其自动化和优化潜力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Advancing Building Energy Modeling with Large Language Models:  Exploration and Case Studies](http://arxiv.org/abs/2402.09579v2)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种基于框架分类的模型无关框架，通过标签增强技术和输出帧选择策略，以监督方式提升词边界检测性能。", "part1分数": "4", "part2分数": "4", "part3分数": "4", "part4分数": "4", "part5分数": "4", "part6分数": "4", "推荐分数": 4.0, "标题": "[Back to Supervision: Boosting Word Boundary Detection through Frame  Classification](http://arxiv.org/abs/2411.10423v1)", "领域": {"语音理解": ["speech processing"]}}
{"论文摘要": "分析私有数据下闭源LLM的隐私保护和性能，提出使用开源LLM实现高性能且成本更低的隐私保护LLM适应方案。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Open LLMs are Necessary for Current Private Adaptations and Outperform  their Closed Alternatives](http://arxiv.org/abs/2411.05818v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出ThermoHands基准，用于评估基于热成像的3D手部姿态估计的鲁棒性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric  Thermal Images](http://arxiv.org/abs/2403.09871v4)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种结构化剪枝和脉冲阵列协同设计框架，以提高边缘系统中Transformer的效率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Systolic Arrays and Structured Pruning Co-design for Efficient  Transformers in Edge Systems](http://arxiv.org/abs/2411.10285v1)", "领域": {"语音理解": ["speech recognition"], "大模型": ["transformer"]}}
{"论文摘要": "提出了一种针对剪枝后大型语言模型的训练数据量优化方法，通过建立缩放定律预测模型损失，以实现性能恢复。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Scaling Law for Post-training after Model Pruning](http://arxiv.org/abs/2411.10272v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs", "transformer"]}}
{"论文摘要": "提出了一种名为RETR的多视图雷达检测Transformer模型，用于室内感知，通过改进DETR架构以适应多视图雷达设置，并在室内雷达感知数据集上取得了优于现有方法的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[RETR: Multi-View Radar Detection Transformer for Indoor Perception](http://arxiv.org/abs/2411.10293v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种多维字节对编码方法，通过将字节对编码从一维扩展到多维，以压缩视觉数据，从而提高视觉数据生成中transformer的训练和推理性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Multidimensional Byte Pair Encoding: Shortened Sequences for Improved  Visual Data Generation](http://arxiv.org/abs/2411.10281v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种基于优化的提示注入攻击方法，针对LLM-as-a-Judge系统，有效绕过现有防御策略。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Optimization-based Prompt Injection Attack to LLM-as-a-Judge](http://arxiv.org/abs/2403.17710v3)", "领域": {"大模型": ["large language model", "LLM"]}}
{"论文摘要": "研究大型语言模型在非对抗性情况下对训练数据的再现能力，并探讨如何减少模型与人类在文本生成上的差异。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.85, "标题": "[Measuring Non-Adversarial Reproduction of Training Data in Large  Language Models](http://arxiv.org/abs/2411.10242v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种名为SLATE的新型时空编码方法，用于动态图上的Transformer，以增强结构化和时序信息的表达能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Supra-Laplacian Encoding for Transformer on Dynamic Graphs](http://arxiv.org/abs/2409.17986v2)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种基于Transformer和TaylorShift的低分辨率图像超分辨率方法，通过使用1x1的补丁大小和TaylorShift注意力机制，显著提高了计算效率并实现了精细图像细节的增强。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image  Super-Resolution with Transformers and TaylorShift](http://arxiv.org/abs/2411.10231v1)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "通过实证研究，评估了基于大型语言模型（LLM）的自动错误修复代理系统的性能，并提出了优化LLM和代理流程设计的建议。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 3, "part6分数": 4, "推荐分数": 3.8, "标题": "[An Empirical Study on LLM-based Agents for Automated Bug Fixing](http://arxiv.org/abs/2411.10213v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "探索大型语言模型（LLMs）在供应链管理中自动化共识寻求的可能性，以解决库存水平和交货时间等决策问题，并降低传统决策中的协调成本。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent  Consensus-Seeking](http://arxiv.org/abs/2411.10184v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "通过比较不同宪法对AI反馈质量的影响，研究AI在医疗访谈中促进以患者为中心的沟通的有效性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 3, "part6分数": 4, "推荐分数": 3.65, "标题": "[Evaluating the role of `Constitutions' for learning from AI feedback](http://arxiv.org/abs/2411.10168v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "通过合成数据干预技术减轻解码器仅有的Transformer架构中的谄媚问题", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Mitigating Sycophancy in Decoder-Only Transformer Architectures:  Synthetic Data Intervention](http://arxiv.org/abs/2411.10156v1)", "领域": {"大模型": ["large language model", "large language models", "transformer"]}}
{"论文摘要": "提出了一种名为DaYu的数据驱动模型，用于高分辨率卫星云图像的短期天气预报，有效解决了现有方法在提供详细短期预报方面的不足。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 5, "part5分数": 4, "part6分数": 5, "推荐分数": 4.65, "标题": "[DaYu: Data-Driven Model for Geostationary Satellite Observed Cloud  Images Forecasting](http://arxiv.org/abs/2411.10144v1)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "通过修改人类认知反射测试问题，研究大型语言模型在解决数学问题时的认知机制，发现其错误率较高，主要依赖模式匹配而非类似人类的推理能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Do Large Language Models Truly Grasp Mathematics? An Empirical  Exploration From Cognitive Psychology](http://arxiv.org/abs/2410.14979v5)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于自回归模型的图像生成新方法，通过迭代添加细节以构建图像，实现了高分辨率图像生成的可扩展性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[CART: Compositional Auto-Regressive Transformer for Image Generation](http://arxiv.org/abs/2411.10180v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "本文通过对比分析不同类型的LLMs在法律领域的应用性能，探讨了LLMs在法律文本理解、法律推理和判决预测等方面的优缺点。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Legal Evalutions and Challenges of Large Language Models](http://arxiv.org/abs/2411.10137v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种扩展Transformer语言模型记忆容量的新方法，通过精确记忆和近似分布记忆的概念，提高了记忆效率并提供了与先前工作的精确比较。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Memorization in Attention-only Transformers](http://arxiv.org/abs/2411.10115v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "本研究提出了一种基于大型语言模型的代码审查评论自动生成方法，通过参数高效的量化低秩（QLoRA）微调和语义元数据信息增强提示，显著提升了代码审查评论生成的准确性和效率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Prompting and Fine-tuning Large Language Models for Automated Code  Review Comment Generation](http://arxiv.org/abs/2411.10129v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "利用大型语言模型模拟1000人的行为态度，以评估其在政策制定和社会科学中的应用潜力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.85, "标题": "[Generative Agent Simulations of 1,000 People](http://arxiv.org/abs/2411.10109v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种通过压缩张量并行LLM推理中的加速器间通信来降低推理延迟的方法。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Communication Compression for Tensor Parallel LLM Inference](http://arxiv.org/abs/2411.09510v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种名为TraceableSpeech的新型文本到语音模型，该模型通过直接生成带水印的语音，提高了水印的不可感知性和语音质量，同时增强了对抗拼接攻击的鲁棒性和操作的时序灵活性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[TraceableSpeech: Towards Proactively Traceable Text-to-Speech with  Watermarking](http://arxiv.org/abs/2406.04840v3)", "领域": {"语音生成": ["TTS"]}}
{"论文摘要": "提出了一种名为Dockformer的新型深度学习分子对接方法，用于大规模虚拟筛选，显著提高了对接准确性和效率。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.4, "标题": "[Dockformer: A transformer-based molecular docking paradigm for  large-scale virtual screening](http://arxiv.org/abs/2411.06740v2)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种名为FedEvPrompt的基于证据学习的联邦学习框架，用于皮肤病变图像分类，通过知识蒸馏和提示学习技术实现隐私保护与性能提升。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Evidential Federated Learning for Skin Lesion Image Classification](http://arxiv.org/abs/2411.10071v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种名为CMATH的跨模态增强Transformer，通过层次变分蒸馏进行多模态情感识别，以解决对话中多模态信息融合的挑战。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[CMATH: Cross-Modality Augmented Transformer with Hierarchical  Variational Distillation for Multimodal Emotion Recognition in Conversation](http://arxiv.org/abs/2411.10060v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "利用大型语言模型从科学文献中挖掘被忽视的气候创新。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Towards unearthing neglected climate innovations from scientific  literature using Large Language Models](http://arxiv.org/abs/2411.10055v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "KuaiFormer：一种基于Transformer的快手内容推荐系统检索框架，通过转向Transformer驱动的下一动作预测范式，显著提升了检索性能。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[KuaiFormer: Transformer-Based Retrieval at Kuaishou](http://arxiv.org/abs/2411.10057v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种基于低秩匹配注意力的轻量级跨模态特征融合方法，用于对话情感识别，有效捕捉情感语义信息并降低计算复杂度。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[A Low-rank Matching Attention based Cross-modal Feature Fusion Method  for Conversational Emotion Recognition](http://arxiv.org/abs/2306.17799v2)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种基于强化学习的迭代训练方法，用于无监督语音识别中的边界分割，显著提升了无监督语音识别的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[REBORN: Reinforcement-Learned Boundary Segmentation with Iterative  Training for Unsupervised ASR](http://arxiv.org/abs/2402.03988v3)", "领域": {"语音理解": ["speech recognition", "asr"]}}
{"论文摘要": "提出了一种基于多模态融合的大型语言模型框架，用于检测和识别短视频中的虚假信息。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying  Misinformation of Short Videos](http://arxiv.org/abs/2411.10032v1)", "领域": {"大模型": ["large language model", "LLM"]}}
{"论文摘要": "提出了一种基于双列架构的Mamba模型，用于检测语音伪造攻击，并通过自监督学习提高模型在有限标注数据下的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing  Attack Detection](http://arxiv.org/abs/2411.10027v1)", "领域": {"语音理解": ["speech recognition"], "大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "利用微调后的Llama 2大语言模型，Jal Anveshak应用程序框架旨在通过多语言和多模态方式帮助印度渔民在沿海地区获得最大鱼获量并解决他们的捕鱼相关问题。", "part1分数": "4", "part2分数": "4", "part3分数": "3", "part4分数": "3", "part5分数": "4", "part6分数": "4", "推荐分数": 3.65, "标题": "[Jal Anveshak: Prediction of fishing zones using fine-tuned LlaMa 2](http://arxiv.org/abs/2411.10050v1)", "领域": {"大模型": ["large language model"]}}
{"论文摘要": "提出了一种名为VLEU的新方法，用于评估文本到图像模型的可泛化性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[VLEU: a Method for Automatic Evaluation for Generalizability of  Text-to-Image Models](http://arxiv.org/abs/2409.14704v2)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种基于Householder反射的低秩和正交自适应方法，以实现大规模预训练模型在特定任务或领域的有效适应。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Bridging The Gap between Low-rank and Orthogonal Adaptation via  Householder Reflection Adaptation](http://arxiv.org/abs/2405.17484v3)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种简单但强大的声音视频生成基线，通过有效融合音频和视频扩散模型，并引入时间步调整和跨模态条件作为位置编码的新机制，实现了音频和视频的联合生成。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[A Simple but Strong Baseline for Sounding Video Generation: Effective  Adaptation of Audio and Video Diffusion Models for Joint Generation](http://arxiv.org/abs/2409.17550v2)", "领域": {"大模型": ["attention mechanism"]}}
{"论文摘要": "提出使用Transformer架构构建6G无线电基础模型，通过自监督学习进行预训练，并在多个下游任务中展示其有效性和泛化能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Building 6G Radio Foundation Models with Transformer Architectures](http://arxiv.org/abs/2411.09996v1)", "领域": {"大模型": ["foundation model", "transformer"]}}
{"论文摘要": "Orca：通过整合个性特征提升大型语言模型的角色扮演能力", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Orca: Enhancing Role-Playing Abilities of Large Language Models by  Integrating Personality Traits](http://arxiv.org/abs/2411.10006v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "评估并增强基于知识图谱的对话推理大型语言模型", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Evaluating and Enhancing Large Language Models for Conversational  Reasoning on Knowledge Graphs](http://arxiv.org/abs/2312.11282v3)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "通过增强注意力头减轻多模态大型语言模型中的幻觉问题", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate  Hallucination in LVLMs](http://arxiv.org/abs/2411.09968v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "利用大型语言模型创建用户代理以评估面向任务的对话系统，并提出了自动评估方法。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Large Language Models as User-Agents for Evaluating  Task-Oriented-Dialogue Systems](http://arxiv.org/abs/2411.09972v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "设计了一种名为TranSlider的AI工具，通过个性化翻译科学文本，以提升不同背景受众对科学内容的理解。", "part1分数": "4", "part2分数": "4", "part3分数": "3", "part4分数": "3", "part5分数": "3", "part6分数": "4", "推荐分数": 3.45, "标题": "[Steering AI-Driven Personalization of Scientific Text for General  Audiences](http://arxiv.org/abs/2411.09969v1)", "领域": {"大模型": ["LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于特征解耦和增强的零样本歌唱声音转换模型SaMoye，能够实现歌唱声音到人类和非人类音色的转换。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature  Disentanglement and Enhancement](http://arxiv.org/abs/2407.07728v5)", "领域": {"语音理解": ["asr"], "语音生成": ["voice conversion"]}}
{"论文摘要": "在LLM时代，本文综述了基于指令的图像和多媒体编辑技术，探讨了如何利用LLM和跨模态模型实现直观的视觉内容编辑。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Instruction-Guided Editing Controls for Images and Multimedia: A Survey  in LLM era](http://arxiv.org/abs/2411.09955v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "TEESlice：在攻击者拥有预训练模型的情况下，通过新型分区策略保护敏感神经网络模型的安全。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[TEESlice: Protecting Sensitive Neural Network Models in Trusted  Execution Environments When Attackers have Pre-Trained Models](http://arxiv.org/abs/2411.09945v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "Zero-shot voice conversion aims to transform a source speech utterance to match the timbre of a reference speech from an unseen speaker.", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Zero-shot Voice Conversion with Diffusion Transformers](http://arxiv.org/abs/2411.09943v1)", "领域": {"语音生成": ["voice conversion"], "大模型": ["transformer"]}}
{"论文摘要": "提出了一种动态的基于大型语言模型（LLM）的智能体网络（DyLAN），用于任务导向的智能体协作，通过团队优化和任务解决两个阶段，实现智能体之间的动态协作。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[A Dynamic LLM-Powered Agent Network for Task-Oriented Agent  Collaboration](http://arxiv.org/abs/2310.02170v2)", "领域": {"大模型": ["large language model", "LLM"]}}
{"论文摘要": "探讨生成式AI中人类-算法混合模型（ centaurs ）在决策制定中的优势及其应用场景。", "part1分数": 4, "part2分数": 3, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.45, "标题": "[Effective Generative AI: The Human-Algorithm Centaur](http://arxiv.org/abs/2406.10942v3)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于进化优化模型融合的日本放射学报告生成模型（JRadiEvo），该模型能够高效利用少量数据生成准确的报告，并适用于对隐私和安全要求严格的医院环境。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by  Evolutionary Optimization of Model Merging](http://arxiv.org/abs/2411.09933v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs", "foundation model"]}}
{"论文摘要": "提出了一种名为ORLM的可定制框架，用于训练大型模型以实现自动优化建模，并通过半自动数据合成框架OR-Instruct和IndustryOR基准测试验证了其性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[ORLM: A Customizable Framework in Training Large Models for Automated  Optimization Modeling](http://arxiv.org/abs/2405.17743v3)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于块坐标下降框架和Adam更新规则的BAdam优化方法，用于大型语言模型的参数微调，并验证了其在内存使用、运行时间和优化能力方面的效率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[BAdam: A Memory Efficient Full Parameter Optimization Method for Large  Language Models](http://arxiv.org/abs/2404.02827v3)", "领域": {"大模型": ["LLM", "LLMs"]}}
{"论文摘要": "AMXFP4：通过非对称微缩放和4位浮点数格式解决大型语言模型推理中的激活异常问题。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[AMXFP4: Taming Activation Outliers with Asymmetric Microscaling  Floating-Point for 4-bit LLM Inference](http://arxiv.org/abs/2411.09909v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种细粒度文本对齐清理器（TA-Cleaner）来增强对比学习中的后门防御能力，有效提升了模型对数据中毒攻击的防御性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[CleanerCLIP: Fine-grained Counterfactual Semantic Augmentation for  Backdoor Defense in Contrastive Learning](http://arxiv.org/abs/2409.17601v3)", "领域": {"语音理解": ["asr"]}}
{"论文摘要": "提出了一种基于知识条件化的大型语言模型，用于从临床和医学影像报告中自动提取肺病变信息，以提高诊断的准确性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Automated Clinical Data Extraction with Knowledge Conditioned LLMs](http://arxiv.org/abs/2406.18027v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种混合人工智能系统，用于自动分析脑电图背景活动并生成报告，以提高小型医院和诊所的诊断准确性。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.4, "标题": "[A Hybrid Artificial Intelligence System for Automated EEG Background  Analysis and Report Generation](http://arxiv.org/abs/2411.09874v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "EHRMamba：一种基于Mamba架构的通用且可扩展的基础模型，用于处理电子健康记录数据，并实现多任务学习和跨任务泛化。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 5, "part5分数": 5, "part6分数": 5, "推荐分数": 4.85, "标题": "[EHRMamba: Towards Generalizable and Scalable Foundation Models for  Electronic Health Records](http://arxiv.org/abs/2405.14567v3)", "领域": {"大模型": ["foundation model", "transformer"]}}
{"论文摘要": "提出了一种针对多模态大型语言模型幻觉问题的针对性直接偏好优化方法，有效降低了模型幻觉的发生。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Mitigating Hallucination in Multimodal Large Language Model via  Hallucination-targeted Direct Preference Optimization](http://arxiv.org/abs/2411.10436v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种新的扩散后验采样方法，通过整合精心设计的测量来提高逆问题的解算能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Enhancing Diffusion Posterior Sampling for Inverse Problems by  Integrating Crafted Measurements](http://arxiv.org/abs/2411.09850v1)", "领域": {"大模型": ["foundation model"]}}
{"论文摘要": "本文旨在为医疗保健专业人员提供使用大型语言模型（LLMs）的实用指南，包括任务制定、模型选择、提示工程、微调和部署等步骤，以确保LLMs在医疗实践中的安全、可靠和有效应用。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 4, "part5分数": 3, "part6分数": 4, "推荐分数": 3.6, "标题": "[Demystifying Large Language Models for Medicine: A Primer](http://arxiv.org/abs/2410.18856v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于Transformer和TaylorShift的低分辨率图像超分辨率方法，通过1x1的patch size和TaylorShift注意力机制，显著提高了超分辨率性能并降低了内存消耗。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[A Low-Resolution Image is Worth 1x1 Words: Enabling Fine Image  Super-Resolution with Transformers and TaylorShift](http://arxiv.org/abs/2411.10231v1)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种名为CMATH的多模态情感识别模型，通过跨模态增强和分层变分蒸馏技术，提高了对话中多模态情感识别的准确性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[CMATH: Cross-Modality Augmented Transformer with Hierarchical  Variational Distillation for Multimodal Emotion Recognition in Conversation](http://arxiv.org/abs/2411.10060v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种名为ConSmax的硬件友好型softmax替代方案，通过可学习的参数实现高效的软件-硬件协同设计，显著降低能耗和面积。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters](http://arxiv.org/abs/2402.10930v3)", "领域": {"大模型": ["LLM", "LLMs", "transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种名为低秩匹配注意力方法（LMAM）的轻量级跨模态特征融合方法，用于对话情感识别，有效捕捉情感语义信息并降低计算复杂度。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[A Low-rank Matching Attention based Cross-modal Feature Fusion Method  for Conversational Emotion Recognition](http://arxiv.org/abs/2306.17799v2)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种基于强化学习的迭代训练方法，用于无监督语音识别中的边界分割，显著提升了无监督语音识别的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[REBORN: Reinforcement-Learned Boundary Segmentation with Iterative  Training for Unsupervised ASR](http://arxiv.org/abs/2402.03988v3)", "领域": {"语音理解": ["speech recognition", "asr"]}}
{"论文摘要": "提出了一种基于特征解耦和增强的零样本歌唱声音转换模型SaMoye，实现了从歌唱声音到人类和非人类音色的转换。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature  Disentanglement and Enhancement](http://arxiv.org/abs/2407.07728v5)", "领域": {"语音理解": ["asr"], "语音生成": ["voice conversion"]}}
{"论文摘要": "提出了一种名为TraceableSpeech的新型文本到语音模型，通过直接生成带水印的语音，提高了水印的不可感知性和语音质量，同时增强了对抗拼接攻击的鲁棒性和操作的时序灵活性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[TraceableSpeech: Towards Proactively Traceable Text-to-Speech with  Watermarking](http://arxiv.org/abs/2406.04840v3)", "领域": {"语音生成": ["TTS"]}}
{"论文摘要": "在LLM时代，本文对基于指令的图像和多媒体编辑技术进行了综述，探讨了如何利用LLM和跨模态模型实现直观的视觉内容编辑。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Instruction-Guided Editing Controls for Images and Multimedia: A Survey  in LLM era](http://arxiv.org/abs/2411.09955v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种名为Seed-VC的零样本语音转换框架，通过引入外部音色转换器和使用扩散变换器来提高转换质量。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.4, "标题": "[Zero-shot Voice Conversion with Diffusion Transformers](http://arxiv.org/abs/2411.09943v1)", "领域": {"语音生成": ["voice conversion"], "大模型": ["transformer"]}}
{"论文摘要": "通过混合偏好优化提升多模态大型语言模型推理能力", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Enhancing the Reasoning Ability of Multimodal Large Language Models via  Mixed Preference Optimization](http://arxiv.org/abs/2411.10442v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于双列架构的Mamba模型，用于检测语音伪造攻击，并通过自监督学习和预训练wav2vec 2.0模型提高其性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[XLSR-Mamba: A Dual-Column Bidirectional State Space Model for Spoofing  Attack Detection](http://arxiv.org/abs/2411.10027v1)", "领域": {"语音理解": ["speech recognition"], "大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "VeriGraph通过整合视觉语言模型（VLMs）进行机器人任务规划，并使用场景图来验证和优化动作序列，显著提高了任务完成率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[VeriGraph: Scene Graphs for Execution Verifiable Robot Planning](http://arxiv.org/abs/2411.10446v1)", "领域": {"大模型": ["LLM"]}}
{"论文摘要": "提出了一种简单但强大的声音视频生成基线，通过有效融合音频和视频扩散模型以及引入时间步调整和跨模态条件作为位置编码的新机制，实现了音频和视频的联合生成。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[A Simple but Strong Baseline for Sounding Video Generation: Effective  Adaptation of Audio and Video Diffusion Models for Joint Generation](http://arxiv.org/abs/2409.17550v2)", "领域": {"大模型": ["attention mechanism"]}}
{"论文摘要": "提出MARS优化框架，通过结合预条件梯度方法和方差减少技术，显著提升大型模型的训练效率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[MARS: Unleashing the Power of Variance Reduction for Training Large  Models](http://arxiv.org/abs/2411.10438v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种针对多模态大型语言模型幻觉问题的针对性直接偏好优化方法，有效降低了幻觉发生的概率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Mitigating Hallucination in Multimodal Large Language Model via  Hallucination-targeted Direct Preference Optimization](http://arxiv.org/abs/2411.10436v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于框架分类的模型无关框架，通过标签增强技术和输出帧选择策略，在监督方式下进行词边界检测，并在Buckeye和TIMIT数据集上取得了新的最先进性能。", "part1分数": 4, "part2分数": 4, "part3分数": 5, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Back to Supervision: Boosting Word Boundary Detection through Frame  Classification](http://arxiv.org/abs/2411.10423v1)", "领域": {"语音理解": ["speech processing"]}}
{"论文摘要": "通过模拟Balderdash游戏，评估大型语言模型在创造性和逻辑推理方面的能力。", "part1分数": "4", "part2分数": "4", "part3分数": "4", "part4分数": "4", "part5分数": "4", "part6分数": "4", "推荐分数": 4.0, "标题": "[Evaluating Creativity and Deception in Large Language Models: A  Simulation Framework for Multi-Agent Balderdash](http://arxiv.org/abs/2411.10422v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "利用大型语言模型和Shapley值原理，开发了一种可解释的机器学习控制框架，以增强建筑能源系统中的机器学习控制的透明度和可信度。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Large Language Model-Based Interpretable Machine Learning Control in  Building Energy Systems](http://arxiv.org/abs/2402.09584v2)", "领域": {"大模型": ["large language model", "LLM", "LLMs"]}}
{"论文摘要": "Llama Guard 3 Vision：一种基于多模态LLM的图像理解安全防护工具，用于保障人机对话中的内容安全。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Llama Guard 3 Vision: Safeguarding Human-AI Image Understanding  Conversations](http://arxiv.org/abs/2411.10414v1)", "领域": {"大模型": ["LLM"]}}
{"论文摘要": "探索大型语言模型在建筑能耗模拟中的应用及其潜力。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 3.8, "标题": "[Advancing Building Energy Modeling with Large Language Models:  Exploration and Case Studies](http://arxiv.org/abs/2402.09579v2)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种名为RETR的多视角雷达检测Transformer模型，用于室内感知，显著提升了检测和分割性能。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[RETR: Multi-View Radar Detection Transformer for Indoor Perception](http://arxiv.org/abs/2411.10293v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "分析并比较了四种私有数据下闭源LLM的私有自适应方法，发现开源LLM在隐私保护和性能上更优。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Open LLMs are Necessary for Current Private Adaptations and Outperform  their Closed Alternatives](http://arxiv.org/abs/2411.05818v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于结构化剪枝和脉冲阵列协同设计的框架，以优化边缘系统中Transformer的效率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Systolic Arrays and Structured Pruning Co-design for Efficient  Transformers in Edge Systems](http://arxiv.org/abs/2411.10285v1)", "领域": {"语音理解": ["speech recognition"], "大模型": ["transformer"]}}
{"论文摘要": "通过AI审计，研究GPT-3.5在招聘过程中存在的种族和性别偏见。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.85, "标题": "[The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring](http://arxiv.org/abs/2405.04412v3)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出ThermoHands基准，用于评估基于热成像的3D手部姿态估计的鲁棒性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric  Thermal Images](http://arxiv.org/abs/2403.09871v4)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种多维字节对编码方法，通过将字节对编码从一维扩展到多维，以改善视觉数据的生成。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Multidimensional Byte Pair Encoding: Shortened Sequences for Improved  Visual Data Generation](http://arxiv.org/abs/2411.10281v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种针对剪枝后大型语言模型的训练数据量优化方法，通过建立缩放定律预测模型损失，以实现性能恢复。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Scaling Law for Post-training after Model Pruning](http://arxiv.org/abs/2411.10272v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs", "transformer"]}}
{"论文摘要": "提出了一种基于优化的提示注入攻击方法，针对LLM-as-a-Judge系统，有效绕过现有防御策略。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Optimization-based Prompt Injection Attack to LLM-as-a-Judge](http://arxiv.org/abs/2403.17710v3)", "领域": {"大模型": ["large language model", "LLM"]}}
{"论文摘要": "研究大型语言模型在非对抗性条件下对训练数据的重放现象，并探讨减少这种重放的方法。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.85, "标题": "[Measuring Non-Adversarial Reproduction of Training Data in Large  Language Models](http://arxiv.org/abs/2411.10242v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种名为SLATE的动态图Transformer编码方法，通过超拉普拉斯编码和交叉注意力机制，在动态链接预测任务中优于现有方法。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Supra-Laplacian Encoding for Transformer on Dynamic Graphs](http://arxiv.org/abs/2409.17986v2)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "本研究评估了基于大型语言模型的自动缺陷修复代理系统的性能，并分析了其优化需求。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[An Empirical Study on LLM-based Agents for Automated Bug Fixing](http://arxiv.org/abs/2411.10213v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "本文探讨了大型语言模型（LLMs）在供应链管理中自动化共识寻求的可能性，并提出了一种针对LLM代理的供应链特定共识寻求框架。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Agentic LLMs in the Supply Chain: Towards Autonomous Multi-Agent  Consensus-Seeking](http://arxiv.org/abs/2411.10184v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于自回归模型的图像生成方法，通过迭代添加细节以构建图像，提高了图像生成的保真度和可扩展性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[CART: Compositional Auto-Regressive Transformer for Image Generation](http://arxiv.org/abs/2411.10180v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "研究通过对比不同宪法对AI反馈质量的影响，评估其在医疗访谈中提升患者中心沟通的效果。", "part1分数": "4", "part2分数": "4", "part3分数": "4", "part4分数": "3", "part5分数": "3", "part6分数": "4", "推荐分数": 3.65, "标题": "[Evaluating the role of `Constitutions' for learning from AI feedback](http://arxiv.org/abs/2411.10168v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种针对复合问题的基准测试，用于评估大型语言模型在理解和推理方面的能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Compound-QA: A Benchmark for Evaluating LLMs on Compound Questions](http://arxiv.org/abs/2411.10163v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "通过合成数据干预技术减轻仅解码器Transformer架构中的谄媚问题", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 3, "part6分数": 4, "推荐分数": 3.65, "标题": "[Mitigating Sycophancy in Decoder-Only Transformer Architectures:  Synthetic Data Intervention](http://arxiv.org/abs/2411.10156v1)", "领域": {"大模型": ["large language model", "large language models", "transformer"]}}
{"论文摘要": "通过修改人类认知反射测试问题，研究大型语言模型在解决数学问题时的认知机制，发现其错误率较高，主要依赖模式匹配而非类似人类的推理能力。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 3, "part6分数": 4, "推荐分数": 3.45, "标题": "[Do Large Language Models Truly Grasp Mathematics? An Empirical  Exploration From Cognitive Psychology](http://arxiv.org/abs/2410.14979v5)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种有效框架，帮助大型语言模型处理涉及数值的长文本任务，通过分解任务并利用代码生成技术提高准确性和降低API调用成本。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.85, "标题": "[An Effective Framework to Help Large Language Models Handle  Numeric-involved Long-context Tasks](http://arxiv.org/abs/2411.10145v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种名为DaYu的数据驱动模型，用于高分辨率卫星云图短期天气预报，有效解决现有方法的不足。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.4, "标题": "[DaYu: Data-Driven Model for Geostationary Satellite Observed Cloud  Images Forecasting](http://arxiv.org/abs/2411.10144v1)", "领域": {"大模型": ["transformer", "attention mechanism"]}}
{"论文摘要": "提出了一种基于大语言模型的代码审查评论生成方法，通过参数高效、量化低秩（QLoRA）微调和提示增强技术，显著提升了代码审查评论的生成性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Prompting and Fine-tuning Large Language Models for Automated Code  Review Comment Generation](http://arxiv.org/abs/2411.10129v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "利用大型语言模型模拟1000人的行为态度，以评估其在政策制定和社会科学中的应用潜力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Generative Agent Simulations of 1,000 People](http://arxiv.org/abs/2411.10109v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种通过压缩张量并行LLM推理中的加速器间通信来降低推理延迟的方法。", "part1分数": "4", "part2分数": "4", "part3分数": "4", "part4分数": "3", "part5分数": "4", "part6分数": "4", "推荐分数": 3.85, "标题": "[Communication Compression for Tensor Parallel LLM Inference](http://arxiv.org/abs/2411.09510v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种新的证明方法，扩展了基于语言的Transformer的上下文大小假设，并引入了分布的近似记忆概念，以更有效地实现精确记忆。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Memorization in Attention-only Transformers](http://arxiv.org/abs/2411.10115v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "Xmodel-1.5是一个基于2万亿语料库预训练的1亿参数多语言大型语言模型，在泰语、阿拉伯语、法语等语言上表现出色，并贡献了一个新的泰语评估数据集。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.85, "标题": "[Xmodel-1.5: An 1B-scale Multilingual LLM](http://arxiv.org/abs/2411.10083v1)", "领域": {"大模型": ["LLM"]}}
{"论文摘要": "通过增强激活方差-稀疏度评分，分析大型语言模型中层的重要性及幻觉倾向，并提出一种新的方法来减少幻觉生成。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Layer Importance and Hallucination Analysis in Large Language Models via  Enhanced Activation Variance-Sparsity](http://arxiv.org/abs/2411.10069v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种名为FedEvPrompt的基于证据学习的联邦学习框架，用于皮肤病变图像分类，通过知识蒸馏和注意力图共享实现隐私保护。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Evidential Federated Learning for Skin Lesion Image Classification](http://arxiv.org/abs/2411.10071v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种名为CMATH的跨模态增强Transformer，通过分层变分蒸馏和不对称融合策略，提高对话中多模态情感识别的准确性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[CMATH: Cross-Modality Augmented Transformer with Hierarchical  Variational Distillation for Multimodal Emotion Recognition in Conversation](http://arxiv.org/abs/2411.10060v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "通过实证分析评估了三种获取分布的方法，探讨了温度对模型与人类意见对齐的影响，并指出模型反映人类意见的局限性。", "part1分数": "4", "part2分数": "4", "part3分数": "3", "part4分数": "3", "part5分数": "3", "part6分数": "4", "推荐分数": 3.45, "标题": "[Understanding The Effect Of Temperature On Alignment With Human Opinions](http://arxiv.org/abs/2411.10080v1)", "领域": {"大模型": ["LLM", "LLMs"]}}
{"论文摘要": "KuaiFormer是一种基于Transformer的检索框架，用于大规模内容推荐系统，通过转换传统评分估计任务为Transformer驱动的下一动作预测范式，显著提升检索性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[KuaiFormer: Transformer-Based Retrieval at Kuaishou](http://arxiv.org/abs/2411.10057v1)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "利用大型语言模型从科学文献中挖掘被忽视的气候创新。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Towards unearthing neglected climate innovations from scientific  literature using Large Language Models](http://arxiv.org/abs/2411.10055v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "Dockformer：一种基于Transformer的分子对接范式，用于大规模虚拟筛选，显著提高了对接准确性和效率。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Dockformer: A transformer-based molecular docking paradigm for  large-scale virtual screening](http://arxiv.org/abs/2411.06740v2)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "提出了一种基于强化学习的迭代训练方法，用于无监督语音识别中的边界分割，显著提升了无监督语音识别的性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[REBORN: Reinforcement-Learned Boundary Segmentation with Iterative  Training for Unsupervised ASR](http://arxiv.org/abs/2402.03988v3)", "领域": {"语音理解": ["speech recognition", "asr"]}}
{"论文摘要": "提出了一种基于多模态融合的大型语言模型框架，用于检测和识别短视频中的虚假信息。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[VMID: A Multimodal Fusion LLM Framework for Detecting and Identifying  Misinformation of Short Videos](http://arxiv.org/abs/2411.10032v1)", "领域": {"大模型": ["large language model", "LLM"]}}
{"论文摘要": "利用微调后的Llama 2大型语言模型，开发Jal Anveshak应用框架，以帮助印度渔民在沿海地区安全获得最大鱼获量并解决多语言和多媒体的捕鱼相关问题。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Jal Anveshak: Prediction of fishing zones using fine-tuned LlaMa 2](http://arxiv.org/abs/2411.10050v1)", "领域": {"大模型": ["large language model"]}}
{"论文摘要": "评估了大型语言模型在临床笔记信息提取中的性能，并比较了其与传统深度学习方法的优缺点。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Information Extraction from Clinical Notes: Are We Ready to Switch to  Large Language Models?](http://arxiv.org/abs/2411.10020v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于Householder反射的低秩和正交自适应方法，以实现大规模预训练模型在特定任务或领域的高效适配。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Bridging The Gap between Low-rank and Orthogonal Adaptation via  Householder Reflection Adaptation](http://arxiv.org/abs/2405.17484v3)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "Orca：通过整合个性特征提升大型语言模型的角色扮演能力", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Orca: Enhancing Role-Playing Abilities of Large Language Models by  Integrating Personality Traits](http://arxiv.org/abs/2411.10006v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于音频和视频扩散模型的简单但强大的声音视频生成基线，通过引入时间步调整和跨模态条件作为位置编码（CMC-PE）机制，提高了音频视频对的同步性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[A Simple but Strong Baseline for Sounding Video Generation: Effective  Adaptation of Audio and Video Diffusion Models for Joint Generation](http://arxiv.org/abs/2409.17550v2)", "领域": {"大模型": ["attention mechanism"]}}
{"论文摘要": "提出了一种名为VLEU的新方法，用于评估文本到图像模型的可泛化性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[VLEU: a Method for Automatic Evaluation for Generalizability of  Text-to-Image Models](http://arxiv.org/abs/2409.14704v2)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出使用Transformer架构构建6G无线通信的射频基础模型，并通过掩码谱图建模进行自监督预训练，以提升模型适应性和性能。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[Building 6G Radio Foundation Models with Transformer Architectures](http://arxiv.org/abs/2411.09996v1)", "领域": {"大模型": ["foundation model", "transformer"]}}
{"论文摘要": "提出了一种基于特征解耦和增强的零样本歌唱声音转换模型SaMoye，实现了从歌唱声音到人类和非人类音色的转换。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[SaMoye: Zero-shot Singing Voice Conversion Model Based on Feature  Disentanglement and Enhancement](http://arxiv.org/abs/2407.07728v5)", "领域": {"语音理解": ["asr"], "语音生成": ["voice conversion"]}}
{"论文摘要": "评估和提升基于知识图谱的大语言模型在对话推理上的性能", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Evaluating and Enhancing Large Language Models for Conversational  Reasoning on Knowledge Graphs](http://arxiv.org/abs/2312.11282v3)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "揭示了长上下文任务难以完成的原因，即“多匹配检索”和“基于逻辑的检索”两个问题，并指出这些问题超出了现有长上下文语言模型的能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Hyper-multi-step: The Truth Behind Difficult Long-context Tasks](http://arxiv.org/abs/2410.04422v5)", "领域": {"大模型": ["LLM", "LLMs"]}}
{"论文摘要": "设计了一种名为TranSlider的AI工具，通过个性化翻译科学文本，提高科学知识在普通受众中的传播效果。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Steering AI-Driven Personalization of Scientific Text for General  Audiences](http://arxiv.org/abs/2411.09969v1)", "领域": {"大模型": ["LLM", "LLMs"]}}
{"论文摘要": "利用大型语言模型创建用户代理以评估面向任务的对话系统，并改进了评估方法和性能。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Large Language Models as User-Agents for Evaluating  Task-Oriented-Dialogue Systems](http://arxiv.org/abs/2411.09972v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "HistoLens：一种基于大型语言模型的多层分析框架，用于历史文本分析，以《盐铁论》为例，展示了其在历史研究和教育中的应用。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[HistoLens: An LLM-Powered Framework for Multi-Layered Analysis of  Historical Texts -- A Case Application of Yantie Lun](http://arxiv.org/abs/2411.09978v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "通过增强注意力头部，缓解多模态大型语言模型中的幻觉问题。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate  Hallucination in LVLMs](http://arxiv.org/abs/2411.09968v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "在大型语言模型时代，本文综述了基于指令的图像和多媒体编辑技术，探讨了如何利用LLMs和跨模态模型实现直观的视觉内容编辑。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Instruction-Guided Editing Controls for Images and Multimedia: A Survey  in LLM era](http://arxiv.org/abs/2411.09955v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "TEESlice：在攻击者拥有预训练模型的情况下，通过新型分区策略保护敏感神经网络模型的安全。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[TEESlice: Protecting Sensitive Neural Network Models in Trusted  Execution Environments When Attackers have Pre-Trained Models](http://arxiv.org/abs/2411.09945v1)", "领域": {"大模型": ["large language model", "large language models"]}}
{"论文摘要": "提出了一种动态的基于大型语言模型（LLM）的智能体网络（DyLAN），用于任务导向的智能体协作，通过团队优化和任务解决两个阶段实现智能体的动态协作。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[A Dynamic LLM-Powered Agent Network for Task-Oriented Agent  Collaboration](http://arxiv.org/abs/2310.02170v2)", "领域": {"大模型": ["large language model", "LLM"]}}
{"论文摘要": "提出Seed-VC，一种基于扩散变换器的零样本语音转换框架，有效解决传统方法中的音色泄露、音色表示不足和训练与推理任务不匹配等问题。", "part1分数": 4, "part2分数": 5, "part3分数": 5, "part4分数": 5, "part5分数": 4, "part6分数": 4, "推荐分数": 4.55, "标题": "[Zero-shot Voice Conversion with Diffusion Transformers](http://arxiv.org/abs/2411.09943v1)", "领域": {"语音生成": ["voice conversion"], "大模型": ["transformer"]}}
{"论文摘要": "利用大型语言模型对日本内阁府经济观察者调查中的价格评论进行分类，构建更精确的价格情绪指数。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.85, "标题": "[Refined and Segmented Price Sentiment Indices from Survey Comments](http://arxiv.org/abs/2411.09937v1)", "领域": {"大模型": ["LLM", "LLMs"]}}
{"论文摘要": "BAdam：一种基于BCD框架和Adam更新规则的内存高效全参数优化方法，用于大型语言模型的微调。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[BAdam: A Memory Efficient Full Parameter Optimization Method for Large  Language Models](http://arxiv.org/abs/2404.02827v3)", "领域": {"大模型": ["LLM", "LLMs"]}}
{"论文摘要": "本文探讨了结合人工智能与人类智能的“人机混合模型”（Centaur）在决策制定中的应用，并分析了其在生成式人工智能，特别是大型语言模型中的关键作用。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 3.8, "标题": "[Effective Generative AI: The Human-Algorithm Centaur](http://arxiv.org/abs/2406.10942v3)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种名为Motion-Grounded Video Reasoning的新方法，通过视频推理进行运动理解，并构建了大规模数据集GROUNDMORE以评估模型在时空定位和推理方面的能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at  Pixel Level](http://arxiv.org/abs/2411.09921v1)", "领域": {"大模型": ["LLM"]}}
{"论文摘要": "提出了一种名为ORLM的可定制框架，用于训练大型模型以实现自动优化建模，并通过半自动数据合成框架OR-Instruct和IndustryOR基准测试展示了其在优化建模方面的卓越性能。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[ORLM: A Customizable Framework in Training Large Models for Automated  Optimization Modeling](http://arxiv.org/abs/2405.17743v3)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种名为AMXFP4的4位浮点数格式，用于解决大规模语言模型低精度量化中的激活异常问题，提高了4位量化下的推理效率。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[AMXFP4: Taming Activation Outliers with Asymmetric Microscaling  Floating-Point for 4-bit LLM Inference](http://arxiv.org/abs/2411.09909v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "探讨了视觉-语言适应对视觉语言模型安全性的影响，并提出了一种权重合并方法以减少安全性退化。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[How Does Vision-Language Adaptation Impact the Safety of Vision Language  Models?](http://arxiv.org/abs/2410.07571v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种基于进化优化模型融合的日本放射学报告生成模型（JRadiEvo），该模型在有限数据下高效生成准确报告，并适用于隐私和安全要求严格的医院环境。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[JRadiEvo: A Japanese Radiology Report Generation Model Enhanced by  Evolutionary Optimization of Model Merging](http://arxiv.org/abs/2411.09933v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs", "foundation model"]}}
{"论文摘要": "提出DriveThru平台，用于从印尼地方语言档案中提取文档内容，并构建基准数据集，以促进印尼语言资源的数字化。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[DriveThru: a Document Extraction Platform and Benchmark Datasets for  Indonesian Local Language Archives](http://arxiv.org/abs/2411.09318v2)", "领域": {"大模型": ["LLM"]}}
{"论文摘要": "提出一种基于知识条件化的大型语言模型，通过上下文学习提高临床数据中肺部病变信息的提取准确性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Automated Clinical Data Extraction with Knowledge Conditioned LLMs](http://arxiv.org/abs/2406.18027v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "CleanerCLIP通过精细的文本对齐清洁器（TA-Cleaner）增强对比学习中的后门防御能力，有效应对数据中毒攻击。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[CleanerCLIP: Fine-grained Counterfactual Semantic Augmentation for  Backdoor Defense in Contrastive Learning](http://arxiv.org/abs/2409.17601v3)", "领域": {"语音理解": ["asr"]}}
{"论文摘要": "提出了一种混合人工智能系统，用于自动分析脑电图背景活动和生成报告，以提高小型医院和诊所的脑电图诊断准确性。", "part1分数": "4", "part2分数": "4", "part3分数": "4", "part4分数": "4", "part5分数": "4", "part6分数": "4", "推荐分数": 4.0, "标题": "[A Hybrid Artificial Intelligence System for Automated EEG Background  Analysis and Report Generation](http://arxiv.org/abs/2411.09874v1)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种名为DeepOSets的非自回归神经网络架构，用于情境下的监督学习算子学习，显著提高了计算效率并减少了噪声敏感性。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[DeepOSets: Non-Autoregressive In-Context Learning of Supervised Learning  Operators](http://arxiv.org/abs/2410.09298v2)", "领域": {"大模型": ["transformer"]}}
{"论文摘要": "EHRMamba：一种基于Mamba架构的通用且可扩展的基础模型，用于处理电子健康记录数据，并实现多任务学习和跨任务泛化。", "part1分数": 4, "part2分数": 5, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.2, "标题": "[EHRMamba: Towards Generalizable and Scalable Foundation Models for  Electronic Health Records](http://arxiv.org/abs/2405.14567v3)", "领域": {"大模型": ["foundation model", "transformer"]}}
{"论文摘要": "提出了一种名为ConSmax的硬件友好的软最大化替代方案，通过可学习的参数减少Softmax计算中的最大值搜索和分母求和，实现高效的并行化处理，同时保持与Softmax相当的准确度。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 5, "part6分数": 4, "推荐分数": 4.2, "标题": "[ConSmax: Hardware-Friendly Alternative Softmax with Learnable Parameters](http://arxiv.org/abs/2402.10930v3)", "领域": {"大模型": ["LLM", "LLMs", "transformer", "attention mechanism"]}}
{"论文摘要": "本文旨在为医疗保健专业人员提供使用大型语言模型（LLMs）的实用指南，以更有效地在临床实践中整合这些技术。", "part1分数": 4, "part2分数": 4, "part3分数": 3, "part4分数": 3, "part5分数": 4, "part6分数": 4, "推荐分数": 3.65, "标题": "[Demystifying Large Language Models for Medicine: A Primer](http://arxiv.org/abs/2410.18856v2)", "领域": {"大模型": ["large language model", "large language models", "LLM", "LLMs"]}}
{"论文摘要": "提出了一种新的扩散后验采样方法，通过整合定制测量来提高逆问题的求解能力。", "part1分数": 4, "part2分数": 4, "part3分数": 4, "part4分数": 4, "part5分数": 4, "part6分数": 4, "推荐分数": 4.0, "标题": "[Enhancing Diffusion Posterior Sampling for Inverse Problems by  Integrating Crafted Measurements](http://arxiv.org/abs/2411.09850v1)", "领域": {"大模型": ["foundation model"]}}
